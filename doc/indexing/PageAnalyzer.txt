Класс PageAnalyzer

На входе через конструктор:
URL - адрес страницы, 
SiteEntity - сайт к которому принадлежит страница, 
LemmaFinder - лемматизатор,
CopyOnWriteArrayList<String> - кэш для адресов просмотренных страниц, 
PageRepository, LemmaRepository, IndexRepository - репо
IndexingServiceImpl - сервис в котором проверяем кнопку "стоп"

все это приходит из SiteAnalyzer

- выводим в лог начало парсинга страницы
- проверяем не прервана ли текущая нить (флаг isInterrupted() текущей нити) и не нажата ли кнопка "стоп" пользователем
(флаг isStop экземпляра IndexingServiceImpl), если какое-то из этих событий произошло, то возвращаем false и выходим
- перед парсингом делаем задержку 200 millis для пауз между запросами
- с помощью библиотеки Jsoup в результате парсинга получаем объект Document
- на основе полученных данных создаем объект Page (пока без id, получим позже при сохранении в БД)
- если код ответа 200 то запускаем процесс анализа содержимого страницы, если код ответа не 200, то пропускаем анализ и
выводим в лог значение кода и url страницы
	анализ страницы: 
		- с помощью лемматизатора из контента страницы извлекаем леммы и их количество в виде Map<String, Float>
		- в результате преобразования Map получаем сэт лемм на основании которого создаем список объектов Lemma(без id)
		- создаем список объектов Index, в которых заполняем пока только поле rank(без полей: id, lemmaId, pageId)
- сохраняем в БД страницу и получаем обратно полноценный объект Page, но уже со всеми, полностью заполненными, полями
- если список объектов Lemma не пустой(в результате анализа страницы), то выполняем код блока synchronized. Так как
в данный момент программа находится в многопоточном режиме (ForkJoinPool), то для того, что бы не возникало конфликтов
или deadlockов с другими нитями при использовании общих объектов, организован блок synchronized. С помощью этого блока
работать с кодом будет только одна нить в один момент.
			В synchronized-блоке:
				- пакетно сохраняем в lemmaRepository список объектов Lemma и получаем в ответ такой же список, но уже
				  c заполненными id
				- предварительно заполнив поля pageId и lemmaId пакетно сохраняем список объектов типа Index
				в indexRepository и получаем в ответ такой же список, но уже со всеми, полностью заполненными, полями
- создаем список задач для парсинга страниц ссылки на которые указаны в контенте страницы, предварительно проверив:
		- ссылка ведет на страницу этого сайта,
		- ссылка ведет на страницу, а не на рисунок или другой документ
		- ссылки нет в кэше ссылок (после проверки добавляем ссылку в кэш)
- передаем задачи в ForkJoinPool
- выводим в лог окончание парсинга страницы
- проверяем не прервана ли текущая нить (флаг isInterrupted() текущей нити) и не нажата ли кнопка стоп(флаг isStop
экземпляра IndexingServiceImpl), если какое-то из этих событий произошло, то возвращаем false если этих событий
не произошло то возвращаем true
--------------------------------------------------------------------------

Еслм в процессе парсинга страницы возникают исключения типа:
 - HttpStatusException - в блоке catch формируем объект типа PageEntity, в поле code заносим e.getStatusCode(),
  остальные поля заполняем соответственно, сохраняем в БД
 - InterruptedException - сообщение выводим в лог и устанавливаем флаг interrupt у текущей нити.
 - IOException - сообщение выводим в лог